{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Копия Фильм_часть3_генерация-фильма-целиком","provenance":[{"file_id":"1r9-lZeOcF_67o4u4NJmcVAC6B2Ib5wI5","timestamp":1630683193726},{"file_id":"1pM33ygbhOjldzNf_hZLeFoaMcbFW4UoO","timestamp":1630511318331},{"file_id":"1uInomyx7kg4DPeTRa7vBJwEnU9wPIzfh","timestamp":1630496510890},{"file_id":"1mk6_uww0Ck6ejBlRhfyFfhbs_Rs7ltSd","timestamp":1629915233195},{"file_id":"1go6YwMFe5MX6XM9tv-cnQiSTU50N9EeT","timestamp":1628548384692},{"file_id":"1L8oL-vLJXVcRzCFbPwOoMkPKJ8-aYdPN","timestamp":1621115030940},{"file_id":"15UwYDsnNeldJFHJ9NdgYBYeo6xPmSelP","timestamp":1618333741979}],"collapsed_sections":["WDe2ZAG_TP4R","YlH6QTVwUCNo","TqKEeBWCUmjQ","1tthw0YaispD","A7UdBeWJPDYc","J-UaaVwwPfYL","aAX4vS4BPp6j","02ZbcWw5YYnU","83x5lq63R_DM"],"machine_shape":"hm"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"4092caad911f4d8b961289f71d075e45":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_493f31b3395f4ddca7533ff77b7e4162","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_a1294f83193848febe4b7a6c87816f06","IPY_MODEL_534246bc28b245278c345a3d1272d373","IPY_MODEL_c05e9cd1c28b49d89339b2d014ee9ff2"]}},"493f31b3395f4ddca7533ff77b7e4162":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"a1294f83193848febe4b7a6c87816f06":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_e8eca0368a454554993a1f3b481e5f3a","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":"100%","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_9642a91e2189471e92aee68365281e9c"}},"534246bc28b245278c345a3d1272d373":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_view_name":"ProgressView","style":"IPY_MODEL_570ec0731ab64ebb941d16a2038e7587","_dom_classes":[],"description":"","_model_name":"FloatProgressModel","bar_style":"success","max":1555,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":1555,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_aea9d5d91ad34820ae5687e0dacdbbf8"}},"c05e9cd1c28b49d89339b2d014ee9ff2":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_6d474411acf7471eab82bb776a4d0520","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 1555/1555 [01:40&lt;00:00, 12.82it/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_c609fb3495744ab9a53bd2831475107d"}},"e8eca0368a454554993a1f3b481e5f3a":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"9642a91e2189471e92aee68365281e9c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"570ec0731ab64ebb941d16a2038e7587":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"aea9d5d91ad34820ae5687e0dacdbbf8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"6d474411acf7471eab82bb776a4d0520":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"c609fb3495744ab9a53bd2831475107d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}}}}},"cells":[{"cell_type":"markdown","metadata":{"id":"CppIQlPhhwhs"},"source":["# Генерация фильма целиком"]},{"cell_type":"markdown","metadata":{"id":"WDe2ZAG_TP4R"},"source":["##Установка библиотек"]},{"cell_type":"code","metadata":{"id":"TkUfzT60ZZ9q"},"source":["!nvidia-smi"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"wSfISAhyPmyp","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1634405906874,"user_tz":-180,"elapsed":70636,"user":{"displayName":"Дарья Курганская","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"10001247416734268166"}},"outputId":"6f8b67c1-6398-4218-aa58-ac5f84c28730"},"source":["print(\"CLIP...\")\n","!git clone https://github.com/openai/CLIP                 &> /dev/null\n"," \n","print(\"Python for IA...\")\n","!git clone https://github.com/CompVis/taming-transformers &> /dev/null\n","!pip install ftfy regex tqdm omegaconf pytorch-lightning  &> /dev/null\n","!pip install kornia                                       &> /dev/null\n","!pip install einops                                       &> /dev/null\n","!pip install wget                                         &> /dev/null\n"," \n","print(\"Others...\")\n","!pip install stegano                                      &> /dev/null\n","!apt install exempi                                       &> /dev/null\n","!pip install python-xmp-toolkit                           &> /dev/null\n","!pip install imgtag                                       &> /dev/null\n","!pip install pillow==7.1.2                                &> /dev/null\n"," \n","print(\"Python to create videos...\")\n","!pip install imageio-ffmpeg                               &> /dev/null\n","!mkdir steps"],"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["CLIP...\n","Python for IA...\n","Others...\n","Python to create videos...\n"]}]},{"cell_type":"code","metadata":{"id":"ic96y-5dDoC_","executionInfo":{"status":"ok","timestamp":1634405933111,"user_tz":-180,"elapsed":26280,"user":{"displayName":"Дарья Курганская","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"10001247416734268166"}}},"source":["import argparse\n","import torch"],"execution_count":2,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"YlH6QTVwUCNo"},"source":["## Для генерации"]},{"cell_type":"code","metadata":{"id":"FhhdWrSxQhwg"},"source":["#@title Загрузка модели\n","\n","!curl -L -o gumbel_8192.yaml -C - 'https://heibox.uni-heidelberg.de/d/2e5662443a6b4307b470/files/?p=%2Fconfigs%2Fmodel.yaml&dl=1' #Gumbel 8192\n","!curl -L -o gumbel_8192.ckpt -C - 'https://heibox.uni-heidelberg.de/d/2e5662443a6b4307b470/files/?p=%2Fckpts%2Flast.ckpt&dl=1' #Gumbel 8192"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Q9qitFLJOd5A","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1634406038771,"user_tz":-180,"elapsed":23489,"user":{"displayName":"Дарья Курганская","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"10001247416734268166"}},"outputId":"4e8d7d19-5f2f-4c0a-f6ec-a4953fa9aa57"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","metadata":{"id":"EXMSuW2EQWsd","executionInfo":{"status":"ok","timestamp":1634406044153,"user_tz":-180,"elapsed":2195,"user":{"displayName":"Дарья Курганская","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"10001247416734268166"}}},"source":["#@title Основные функции\n","\n","import argparse\n","import math\n","from pathlib import Path\n","import sys\n"," \n","sys.path.append('./taming-transformers')\n","from IPython import display\n","from base64 import b64encode\n","from omegaconf import OmegaConf\n","from PIL import Image\n","from taming.models import cond_transformer, vqgan\n","import torch\n","from torch import nn, optim\n","from torch.nn import functional as F\n","from torchvision import transforms\n","from torchvision.transforms import functional as TF\n","from tqdm.notebook import tqdm\n"," \n","from CLIP import clip\n","import kornia.augmentation as K\n","import numpy as np\n","import imageio\n","from PIL import ImageFile, Image\n","from imgtag import ImgTag    # metadatos \n","from libxmp import *         # metadatos\n","import libxmp                # metadatos\n","from stegano import lsb\n","import json\n","ImageFile.LOAD_TRUNCATED_IMAGES = True\n"," \n","def sinc(x):\n","    return torch.where(x != 0, torch.sin(math.pi * x) / (math.pi * x), x.new_ones([]))\n"," \n"," \n","def lanczos(x, a):\n","    cond = torch.logical_and(-a < x, x < a)\n","    out = torch.where(cond, sinc(x) * sinc(x/a), x.new_zeros([]))\n","    return out / out.sum()\n"," \n"," \n","def ramp(ratio, width):\n","    n = math.ceil(width / ratio + 1)\n","    out = torch.empty([n])\n","    cur = 0\n","    for i in range(out.shape[0]):\n","        out[i] = cur\n","        cur += ratio\n","    return torch.cat([-out[1:].flip([0]), out])[1:-1]\n"," \n"," \n","def resample(input, size, align_corners=True):\n","    n, c, h, w = input.shape\n","    dh, dw = size\n"," \n","    input = input.view([n * c, 1, h, w])\n"," \n","    if dh < h:\n","        kernel_h = lanczos(ramp(dh / h, 2), 2).to(input.device, input.dtype)\n","        pad_h = (kernel_h.shape[0] - 1) // 2\n","        input = F.pad(input, (0, 0, pad_h, pad_h), 'reflect')\n","        input = F.conv2d(input, kernel_h[None, None, :, None])\n"," \n","    if dw < w:\n","        kernel_w = lanczos(ramp(dw / w, 2), 2).to(input.device, input.dtype)\n","        pad_w = (kernel_w.shape[0] - 1) // 2\n","        input = F.pad(input, (pad_w, pad_w, 0, 0), 'reflect')\n","        input = F.conv2d(input, kernel_w[None, None, None, :])\n"," \n","    input = input.view([n, c, h, w])\n","    return F.interpolate(input, size, mode='bicubic', align_corners=align_corners)\n"," \n"," \n","class ReplaceGrad(torch.autograd.Function):\n","    @staticmethod\n","    def forward(ctx, x_forward, x_backward):\n","        ctx.shape = x_backward.shape\n","        return x_forward\n"," \n","    @staticmethod\n","    def backward(ctx, grad_in):\n","        return None, grad_in.sum_to_size(ctx.shape)\n"," \n"," \n","replace_grad = ReplaceGrad.apply\n"," \n"," \n","class ClampWithGrad(torch.autograd.Function):\n","    @staticmethod\n","    def forward(ctx, input, min, max):\n","        ctx.min = min\n","        ctx.max = max\n","        ctx.save_for_backward(input)\n","        return input.clamp(min, max)\n"," \n","    @staticmethod\n","    def backward(ctx, grad_in):\n","        input, = ctx.saved_tensors\n","        return grad_in * (grad_in * (input - input.clamp(ctx.min, ctx.max)) >= 0), None, None\n"," \n"," \n","clamp_with_grad = ClampWithGrad.apply\n"," \n"," \n","def vector_quantize(x, codebook):\n","    d = x.pow(2).sum(dim=-1, keepdim=True) + codebook.pow(2).sum(dim=1) - 2 * x @ codebook.T\n","    indices = d.argmin(-1)\n","    x_q = F.one_hot(indices, codebook.shape[0]).to(d.dtype) @ codebook\n","    return replace_grad(x_q, x)\n"," \n"," \n","class Prompt(nn.Module):\n","    def __init__(self, embed, weight=1., stop=float('-inf')):\n","        super().__init__()\n","        self.register_buffer('embed', embed)\n","        self.register_buffer('weight', torch.as_tensor(weight))\n","        self.register_buffer('stop', torch.as_tensor(stop))\n"," \n","    def forward(self, input):\n","        input_normed = F.normalize(input.unsqueeze(1), dim=2)\n","        embed_normed = F.normalize(self.embed.unsqueeze(0), dim=2)\n","        dists = input_normed.sub(embed_normed).norm(dim=2).div(2).arcsin().pow(2).mul(2)\n","        dists = dists * self.weight.sign()\n","        return self.weight.abs() * replace_grad(dists, torch.maximum(dists, self.stop)).mean()\n"," \n"," \n","def parse_prompt(prompt):\n","    vals = prompt.rsplit(':', 2)\n","    vals = vals + ['', '1', '-inf'][len(vals):]\n","    return vals[0], float(vals[1]), float(vals[2])\n"," \n"," \n","class MakeCutouts(nn.Module):\n","    def __init__(self, cut_size, cutn, cut_pow=1.):\n","        super().__init__()\n","        self.cut_size = cut_size\n","        self.cutn = cutn\n","        self.cut_pow = cut_pow\n","        self.augs = nn.Sequential(\n","            K.RandomHorizontalFlip(p=0.5),\n","            # K.RandomSolarize(0.01, 0.01, p=0.7),\n","            K.RandomSharpness(0.3,p=0.4),\n","            K.RandomAffine(degrees=30, translate=0.1, p=0.8, padding_mode='border'),\n","            K.RandomPerspective(0.2,p=0.4),\n","            K.ColorJitter(hue=0.01, saturation=0.01, p=0.7))\n","        self.noise_fac = 0.1\n"," \n"," \n","    def forward(self, input):\n","        sideY, sideX = input.shape[2:4]\n","        max_size = min(sideX, sideY)\n","        min_size = min(sideX, sideY, self.cut_size)\n","        cutouts = []\n","        for _ in range(self.cutn):\n","            size = int(torch.rand([])**self.cut_pow * (max_size - min_size) + min_size)\n","            offsetx = torch.randint(0, sideX - size + 1, ())\n","            offsety = torch.randint(0, sideY - size + 1, ())\n","            cutout = input[:, :, offsety:offsety + size, offsetx:offsetx + size]\n","            cutouts.append(resample(cutout, (self.cut_size, self.cut_size)))\n","        batch = self.augs(torch.cat(cutouts, dim=0))\n","        if self.noise_fac:\n","            facs = batch.new_empty([self.cutn, 1, 1, 1]).uniform_(0, self.noise_fac)\n","            batch = batch + facs * torch.randn_like(batch)\n","        return batch\n"," \n"," \n","def load_vqgan_model(config_path, checkpoint_path):\n","    config = OmegaConf.load(config_path)\n","    if config.model.target == 'taming.models.vqgan.VQModel':\n","        model = vqgan.VQModel(**config.model.params)\n","        model.eval().requires_grad_(False)\n","        model.init_from_ckpt(checkpoint_path)\n","    elif config.model.target == 'taming.models.cond_transformer.Net2NetTransformer':\n","        parent_model = cond_transformer.Net2NetTransformer(**config.model.params)\n","        parent_model.eval().requires_grad_(False)\n","        parent_model.init_from_ckpt(checkpoint_path)\n","        model = parent_model.first_stage_model\n","    elif config.model.target == 'taming.models.vqgan.GumbelVQ':\n","        model = vqgan.GumbelVQ(**config.model.params)\n","        print(config.model.params)\n","        model.eval().requires_grad_(False)\n","        model.init_from_ckpt(checkpoint_path)\n","    else:\n","        raise ValueError(f'unknown model type: {config.model.target}')\n","    del model.loss\n","    return model\n"," \n"," \n","def resize_image(image, out_size):\n","    ratio = image.size[0] / image.size[1]\n","    area = min(image.size[0] * image.size[1], out_size[0] * out_size[1])\n","    size = round((area * ratio)**0.5), round((area / ratio)**0.5)\n","    return image.resize(size, Image.LANCZOS)\n"," \n","def download_img(img_url):\n","    try:\n","        return wget.download(img_url,out=\"input.jpg\")\n","    except:\n","        return"],"execution_count":4,"outputs":[]},{"cell_type":"code","metadata":{"id":"WAfzXQV65DnY"},"source":["#@title Основные функции\n","\n","def synth(z):\n","    if is_gumbel:\n","        z_q = vector_quantize(z.movedim(1, 3), model.quantize.embed.weight).movedim(3, 1)\n","    else:\n","        z_q = vector_quantize(z.movedim(1, 3), model.quantize.embedding.weight).movedim(3, 1)\n","    \n","    return clamp_with_grad(model.decode(z_q).add(1).div(2), 0, 1)\n"," \n","def add_xmp_data(nombrefichero):\n","    imagen = ImgTag(filename=nombrefichero)\n","    imagen.xmp.append_array_item(libxmp.consts.XMP_NS_DC, 'creator', 'VQGAN+CLIP', {\"prop_array_is_ordered\":True, \"prop_value_is_array\":True})\n","    if args.prompts:\n","        imagen.xmp.append_array_item(libxmp.consts.XMP_NS_DC, 'title', \" | \".join(args.prompts), {\"prop_array_is_ordered\":True, \"prop_value_is_array\":True})\n","    else:\n","        imagen.xmp.append_array_item(libxmp.consts.XMP_NS_DC, 'title', 'None', {\"prop_array_is_ordered\":True, \"prop_value_is_array\":True})\n","    imagen.xmp.append_array_item(libxmp.consts.XMP_NS_DC, 'i', str(i), {\"prop_array_is_ordered\":True, \"prop_value_is_array\":True})\n","    imagen.xmp.append_array_item(libxmp.consts.XMP_NS_DC, 'model', nombre_modelo, {\"prop_array_is_ordered\":True, \"prop_value_is_array\":True})\n","    imagen.xmp.append_array_item(libxmp.consts.XMP_NS_DC, 'seed',str(seed) , {\"prop_array_is_ordered\":True, \"prop_value_is_array\":True})\n","    imagen.xmp.append_array_item(libxmp.consts.XMP_NS_DC, 'input_images',str(input_images) , {\"prop_array_is_ordered\":True, \"prop_value_is_array\":True})\n","    #for frases in args.prompts:\n","    #    imagen.xmp.append_array_item(libxmp.consts.XMP_NS_DC, 'Prompt' ,frases, {\"prop_array_is_ordered\":True, \"prop_value_is_array\":True})\n","    imagen.close()\n"," \n","def add_stegano_data(filename):\n","    data = {\n","        \"title\": \" | \".join(args.prompts) if args.prompts else None,\n","        \"notebook\": \"VQGAN+CLIP\",\n","        \"i\": i,\n","        \"model\": nombre_modelo,\n","        \"seed\": str(seed),\n","        \"input_images\": input_images\n","    }\n","    lsb.hide(filename, json.dumps(data)).save(filename)\n"," \n","@torch.no_grad()\n","def checkin(i, losses):\n","    losses_str = ', '.join(f'{loss.item():g}' for loss in losses)\n","    tqdm.write(f'i: {i}, loss: {sum(losses).item():g}, losses: {losses_str}')\n","    out = synth(z)\n","    TF.to_pil_image(out[0].cpu()).save('progress.png')\n","    add_stegano_data('progress.png')\n","    add_xmp_data('progress.png')\n","    display.display(display.Image('progress.png'))\n"," \n","def ascend_txt():\n","    global i\n","    global cuento\n","    global j\n","    global path\n","\n","    out = synth(z)\n","    iii = perceptor.encode_image(normalize(make_cutouts(out))).float()\n","\n","    result = []\n","\n","    if args.init_weight:\n","        result.append(F.mse_loss(z, z_orig) * args.init_weight / 2)\n","\n","    for prompt in pMs:\n","        result.append(prompt(iii))\n","    img = np.array(out.mul(255).clamp(0, 255)[0].cpu().detach().numpy().astype(np.uint8))[:,:,:]\n","    img = np.transpose(img, (1, 2, 0))\n","    if i<200 and i%int(200/(cuento*0.36))==0:\n","      filename = f\"{path}/{j:03}_{i:04}.png\"\n","      imageio.imwrite(filename, np.array(img))\n","      add_stegano_data(filename)\n","      add_xmp_data(filename)\n","    elif i>=200 and i%int(250/(cuento*0.64))==0 :\n","        filename = f\"{path}/{j:03}_{i:04}.png\"\n","        imageio.imwrite(filename, np.array(img))\n","        add_stegano_data(filename)\n","        add_xmp_data(filename)\n","    elif i==450 :\n","        filename = f\"{path}/{j:03}_{i:04}.png\"\n","        imageio.imwrite(filename, np.array(img))\n","        add_stegano_data(filename)\n","        add_xmp_data(filename)\n","    return result\n"," \n","def train(i):\n","    opt.zero_grad()\n","    lossAll = ascend_txt()\n","    if i % args.display_freq == 0:\n","        checkin(i, lossAll)\n","    loss = sum(lossAll)\n","    loss.backward()\n","    opt.step()\n","    with torch.no_grad():\n","        z.copy_(z.maximum(z_min).minimum(z_max))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Rva_unIwrofZ"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"TqKEeBWCUmjQ"},"source":["## Текст истории"]},{"cell_type":"code","metadata":{"id":"EMAEebQ_yCXP"},"source":["data=[{'start': 9.1718820861678,\n","  'text': 'One sunny day, the four wives of the old British consul, the vicar, and a young couple head to the seaside country of Oaxham to attend the wedding of a young man'},\n"," {'start': 12.469115646258503,\n","  'text': ' The three old ladies are all of married social standing'}]\n"," #!!!Вставить таймкод!!!"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"q5dSYh4P_0NQ"},"source":["len(data)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1tthw0YaispD"},"source":["## Генерация"]},{"cell_type":"code","metadata":{"id":"MXvPXjHak33t","executionInfo":{"status":"ok","timestamp":1634406098773,"user_tz":-180,"elapsed":259,"user":{"displayName":"Дарья Курганская","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"10001247416734268166"}}},"source":["path = '/content/drive/MyDrive/aiijc/newFilm/moovienew' #@param {type:'string'}"],"execution_count":5,"outputs":[]},{"cell_type":"code","metadata":{"id":"DZCee2s2O__J"},"source":["tiempo=0\n","\n","\n","for j in range(len(data)):\n","\n","  textos = data[j]['text']\n","  ancho =  256 #размеры картинки\n","  alto =  256\n","  modelo = \"gumbel_8192\" \n","  intervalo_imagenes =  50\n","  if j==0:\n","    imagen_inicial = None\n","  else:\n","    imagen_inicial = f'{path}/{j-1:03}_0450.png' #изначальная картинка\n","  imagenes_objetivo = None\n","  seed = -1\n","  max_iteraciones = 450\n","  input_images = \"\"\n"," \n","  nombre_modelo = \"Gumbel 8192\"    \n","\n","  is_gumbel = True\n","\n","  if seed == -1:\n","    seed = None\n","  if imagen_inicial == \"None\":\n","    imagen_inicial = None\n","  elif imagen_inicial and imagen_inicial.lower().startswith(\"http\"):\n","    imagen_inicial = download_img(imagen_inicial)\n","\n","\n","  if imagenes_objetivo == \"None\" or not imagenes_objetivo:\n","    imagenes_objetivo = []\n","  else:\n","    imagenes_objetivo = imagenes_objetivo.split(\"|\")\n","    imagenes_objetivo = [image.strip() for image in imagenes_objetivo]\n","\n","  if imagen_inicial or imagenes_objetivo != []:\n","    input_images = True\n","\n","  textos = [frase.strip() for frase in textos.split(\"|\")]\n","  if textos == ['']:\n","    textos = []\n","\n","\n","  args = argparse.Namespace(\n","    prompts=textos,\n","    image_prompts=imagenes_objetivo,\n","    noise_prompt_seeds=[],\n","    noise_prompt_weights=[],\n","    size=[ancho, alto],\n","    init_image=imagen_inicial,\n","    init_weight=0.,\n","    clip_model='ViT-B/32',\n","    vqgan_config=f'{modelo}.yaml',\n","    vqgan_checkpoint=f'{modelo}.ckpt',\n","    step_size=0.1,\n","    cutn=64,\n","    cut_pow=1.,\n","    display_freq=intervalo_imagenes,\n","    seed=seed,\n","  )\n","\n","  device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n","  print('Using device:', device)\n","  if textos:\n","    print('Using texts:', textos)\n","  if imagenes_objetivo:\n","    print('Using image prompts:', imagenes_objetivo)\n","  if args.seed is None:\n","    seed = torch.seed()\n","  else:\n","    seed = args.seed\n","  torch.manual_seed(seed)\n","  print('Using seed:', seed)\n","\n","  model = load_vqgan_model(args.vqgan_config, args.vqgan_checkpoint).to(device)\n","  perceptor = clip.load(args.clip_model, jit=False)[0].eval().requires_grad_(False).to(device)\n","\n","  cut_size = perceptor.visual.input_resolution\n","  if is_gumbel:\n","    e_dim = model.quantize.embedding_dim\n","  else:\n","    e_dim = model.quantize.e_dim\n","\n","  f = 2**(model.decoder.num_resolutions - 1)\n","  make_cutouts = MakeCutouts(cut_size, args.cutn, cut_pow=args.cut_pow)\n","  if is_gumbel:\n","    n_toks = model.quantize.n_embed\n","  else:\n","    n_toks = model.quantize.n_e\n","\n","  toksX, toksY = args.size[0] // f, args.size[1] // f\n","  sideX, sideY = toksX * f, toksY * f\n","  if is_gumbel:\n","    z_min = model.quantize.embed.weight.min(dim=0).values[None, :, None, None]\n","    z_max = model.quantize.embed.weight.max(dim=0).values[None, :, None, None]\n","  else:\n","    z_min = model.quantize.embedding.weight.min(dim=0).values[None, :, None, None]\n","    z_max = model.quantize.embedding.weight.max(dim=0).values[None, :, None, None]\n","\n","  if args.init_image:\n","    pil_image = Image.open(args.init_image).convert('RGB')\n","    pil_image = pil_image.resize((sideX, sideY), Image.LANCZOS)\n","    z, *_ = model.encode(TF.to_tensor(pil_image).to(device).unsqueeze(0) * 2 - 1)\n","  else:\n","    one_hot = F.one_hot(torch.randint(n_toks, [toksY * toksX], device=device), n_toks).float()\n","    if is_gumbel:\n","        z = one_hot @ model.quantize.embed.weight\n","    else:\n","        z = one_hot @ model.quantize.embedding.weight\n","    z = z.view([-1, toksY, toksX, e_dim]).permute(0, 3, 1, 2)\n","  z_orig = z.clone()\n","  z.requires_grad_(True)\n","  opt = optim.Adam([z], lr=args.step_size)\n","\n","  normalize = transforms.Normalize(mean=[0.48145466, 0.4578275, 0.40821073],\n","                                 std=[0.26862954, 0.26130258, 0.27577711])\n","\n","  pMs = []\n","\n","  for prompt in args.prompts:\n","    txt, weight, stop = parse_prompt(prompt)\n","    embed = perceptor.encode_text(clip.tokenize(txt).to(device)).float()\n","    pMs.append(Prompt(embed, weight, stop).to(device))\n","\n","  for prompt in args.image_prompts:\n","    path, weight, stop = parse_prompt(prompt)\n","    img = resize_image(Image.open(path).convert('RGB'), (sideX, sideY))\n","    batch = make_cutouts(TF.to_tensor(img).unsqueeze(0).to(device))\n","    embed = perceptor.encode_image(normalize(batch)).float()\n","    pMs.append(Prompt(embed, weight, stop).to(device))\n","\n","  for seed, weight in zip(args.noise_prompt_seeds, args.noise_prompt_weights):\n","    gen = torch.Generator().manual_seed(seed)\n","    embed = torch.empty([1, perceptor.visual.output_dim]).normal_(generator=gen)\n","    pMs.append(Prompt(embed, weight).to(device))\n","\n","  cuento = data[j]['start']-tiempo\n","  tiempo = data[j]['start']\n","  cuento*=11\n","\n","  i = 0\n","\n","  try:\n","    with tqdm() as pbar:\n","        while True:\n","            train(i)\n","            if i == max_iteraciones:\n","                break\n","            i += 1\n","            pbar.update()\n","  except KeyboardInterrupt:\n","    pass"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"A7UdBeWJPDYc"},"source":["#Увеличить разрешение 1"]},{"cell_type":"markdown","metadata":{"id":"J-UaaVwwPfYL"},"source":["##Инициализация"]},{"cell_type":"code","metadata":{"id":"a84Pf5BVPc5N","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1633456317590,"user_tz":-180,"elapsed":20769,"user":{"displayName":"Дарья Курганская","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"00136910481207940866"}},"outputId":"a3a333a6-bad4-440d-aacd-769c7694a823"},"source":["#@title Установка необходимых пакетов\n","!pip install torch\n","!pip install torchvision\n","!pip install numpy\n","!pip install pillow\n","\n","!gdown https://drive.google.com/uc?id=1Or9gVJKghtBwiVMWaNnYRj3TPAfkvSNa\n","!unzip sr_weights.zip"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (1.9.0+cu102)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch) (3.7.4.3)\n","Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (0.10.0+cu102)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchvision) (1.19.5)\n","Requirement already satisfied: pillow>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision) (7.1.2)\n","Requirement already satisfied: torch==1.9.0 in /usr/local/lib/python3.7/dist-packages (from torchvision) (1.9.0+cu102)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.9.0->torchvision) (3.7.4.3)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (1.19.5)\n","Requirement already satisfied: pillow in /usr/local/lib/python3.7/dist-packages (7.1.2)\n","Downloading...\n","From: https://drive.google.com/uc?id=1Or9gVJKghtBwiVMWaNnYRj3TPAfkvSNa\n","To: /content/sr_weights.zip\n","108MB [00:02, 47.5MB/s]\n","Archive:  sr_weights.zip\n","  inflating: netG_20blocks_comprandom_dsV4_4x_V3bicubic.pth  \n","  inflating: RRDBNet_PSNR_20blocks_comprandom_dsV4_4x_V3bicubic.pth  \n"]}]},{"cell_type":"code","metadata":{"id":"gcNyusVOPHRj","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1633456319346,"user_tz":-180,"elapsed":1787,"user":{"displayName":"Дарья Курганская","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"00136910481207940866"}},"outputId":"15ef753b-acbc-4a6a-c90a-3e1cb6160fcb"},"source":["#@title Загрузка предобученной модели\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torchvision.transforms.functional as FT\n","import math\n","import numpy as np\n","from PIL import Image\n","from collections import OrderedDict\n","\n","class Generator(nn.Module):\n","    def __init__(self, num_rrdb_blocks=16, tanh_output=True, upscale_factor=4):\n","        super(Generator, self).__init__()\n","        \n","        self.num_upsample_blocks = int(math.log2(upscale_factor))\n","        \n","        # First layer\n","        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1)\n","\n","        # 16/23 ResidualInResidualDenseBlock layer\n","        rrdb_blocks = []\n","        for _ in range(num_rrdb_blocks):\n","            rrdb_blocks += [ResidualInResidualDenseBlock(64, 32, 0.2)]\n","        self.Trunk_RRDB = nn.Sequential(*rrdb_blocks)\n","\n","        # Second conv layer post residual blocks\n","        self.conv2 = nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1)\n","\n","        # Upsampling layers\n","        #self.up1 = nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1)\n","        #self.up2 = nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1)\n","        upsample_blocks = []\n","        for _ in range(self.num_upsample_blocks):\n","            upsample_blocks += [InterpolateUpsampleBlock(64,64,kernel_size=3, stride=1, padding=1)]\n","        self.upblocks = nn.Sequential(*upsample_blocks)\n","        \n","        # Next layer after upper sampling\n","        self.conv3 = nn.Sequential(\n","            nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1),\n","            nn.LeakyReLU(negative_slope=0.2, inplace=True)\n","        )\n","        \n","        # Final output layer\n","        if not tanh_output:\n","\n","            self.conv4 = nn.Conv2d(64, 3, kernel_size=3, stride=1, padding=1)\n","        else:\n","            self.conv4 = nn.Sequential(\n","                nn.Conv2d(64, 3, kernel_size=3, stride=1, padding=1),\n","                nn.Tanh()\n","            )\n","\n","    def forward(self, input: torch.Tensor) -> torch.Tensor:\n","        out1 = self.conv1(input)\n","        trunk = self.Trunk_RRDB(out1)\n","        out2 = self.conv2(trunk)\n","        out = torch.add(out1, out2)\n","        out = self.upblocks(out)\n","        out = self.conv3(out)\n","        out = self.conv4(out)\n","\n","        return out\n","\n","\n","class InterpolateUpsampleBlock(nn.Module):\n","    \n","    def __init__(self, in_channels: int, out_channels: int, kernel_size: int, stride: int, padding: int):\n","        super(InterpolateUpsampleBlock, self).__init__()\n","        \n","        self.up = nn.Conv2d(in_channels, out_channels, kernel_size=kernel_size, \n","                            stride=stride, padding=padding)\n","   \n","    def forward(self, input: torch.Tensor) -> torch.Tensor:\n","        return F.leaky_relu(self.up(F.interpolate(input, scale_factor=2, mode=\"bicubic\", align_corners=True)), 0.2, True)\n","    \n","    \n","class ResidualDenseBlock(nn.Module):\n","\n","    def __init__(self, channels: int = 64, growth_channels: int = 32, scale_ratio: float = 0.2):\n","\n","        super(ResidualDenseBlock, self).__init__()\n","        self.conv1 = nn.Sequential(\n","            nn.Conv2d(channels + 0 * growth_channels, growth_channels, kernel_size=3, stride=1, padding=1),\n","            nn.LeakyReLU(negative_slope=0.2, inplace=True)\n","        )\n","        self.conv2 = nn.Sequential(\n","            nn.Conv2d(channels + 1 * growth_channels, growth_channels, kernel_size=3, stride=1, padding=1),\n","            nn.LeakyReLU(negative_slope=0.2, inplace=True)\n","        )\n","        self.conv3 = nn.Sequential(\n","            nn.Conv2d(channels + 2 * growth_channels, growth_channels, kernel_size=3, stride=1, padding=1),\n","            nn.LeakyReLU(negative_slope=0.2, inplace=True)\n","        )\n","        self.conv4 = nn.Sequential(\n","            nn.Conv2d(channels + 3 * growth_channels, growth_channels, kernel_size=3, stride=1, padding=1),\n","            nn.LeakyReLU(negative_slope=0.2, inplace=True)\n","        )\n","        self.conv5 = nn.Conv2d(channels + 4 * growth_channels, channels, kernel_size=3, stride=1, padding=1)\n","\n","        self.scale_ratio = scale_ratio\n","\n","        for m in self.modules():\n","            if isinstance(m, nn.Conv2d):\n","                nn.init.kaiming_normal_(m.weight)\n","                m.weight.data *= 0.1\n","                if m.bias is not None:\n","                    m.bias.data.zero_()\n","            elif isinstance(m, nn.Linear):\n","                nn.init.kaiming_normal_(m.weight)\n","                m.weight.data *= 0.1\n","                if m.bias is not None:\n","                    m.bias.data.zero_()\n","            elif isinstance(m, nn.BatchNorm2d):\n","                nn.init.constant_(m.weight, 1)\n","                nn.init.constant_(m.bias.data, 0.0)\n","\n","    def forward(self, input: torch.Tensor) -> torch.Tensor:\n","        conv1 = self.conv1(input)\n","        conv2 = self.conv2(torch.cat((input, conv1), 1))\n","        conv3 = self.conv3(torch.cat((input, conv1, conv2), 1))\n","        conv4 = self.conv4(torch.cat((input, conv1, conv2, conv3), 1))\n","        conv5 = self.conv5(torch.cat((input, conv1, conv2, conv3, conv4), 1))\n","\n","        return conv5.mul(self.scale_ratio) + input\n","\n","\n","class ResidualInResidualDenseBlock(nn.Module):\n","\n","    def __init__(self, channels: int = 64, growth_channels: int = 32, scale_ratio: float = 0.2):\n","\n","        super(ResidualInResidualDenseBlock, self).__init__()\n","        self.RDB1 = ResidualDenseBlock(channels, growth_channels, scale_ratio)\n","        self.RDB2 = ResidualDenseBlock(channels, growth_channels, scale_ratio)\n","        self.RDB3 = ResidualDenseBlock(channels, growth_channels, scale_ratio)\n","\n","        self.scale_ratio = scale_ratio\n","\n","    def forward(self, input: torch.Tensor) -> torch.Tensor:\n","        out = self.RDB1(input)\n","        out = self.RDB2(out)\n","        out = self.RDB3(out)\n","\n","        return out.mul(self.scale_ratio) + input\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","# Some constants\n","rgb_weights = torch.FloatTensor([65.481, 128.553, 24.966]).to(device)\n","imagenet_mean = torch.FloatTensor([0.485, 0.456, 0.406]).unsqueeze(1).unsqueeze(2)\n","imagenet_std = torch.FloatTensor([0.229, 0.224, 0.225]).unsqueeze(1).unsqueeze(2)\n","imagenet_mean_cuda = torch.FloatTensor([0.485, 0.456, 0.406]).to(device).unsqueeze(0).unsqueeze(2).unsqueeze(3)\n","imagenet_std_cuda = torch.FloatTensor([0.229, 0.224, 0.225]).to(device).unsqueeze(0).unsqueeze(2).unsqueeze(3)\n","\n","\n","def convert_image(img, source, target):\n","    assert source in {'pil', '[0, 1]', '[-1, 1]'}, \"Cannot convert from source format %s!\" % source\n","    assert target in {'pil', '[0, 255]', '[0, 1]', '[-1, 1]', 'imagenet-norm',\n","                      'y-channel'}, \"Cannot convert to target format %s!\" % target\n","\n","    # Convert from source to [0, 1]\n","    if source == 'pil':\n","        img = FT.to_tensor(img)\n","\n","    elif source == '[0, 1]':\n","        pass  # already in [0, 1]\n","\n","    elif source == '[-1, 1]':\n","        img = (img + 1.) / 2.\n","\n","    # Convert from [0, 1] to target\n","    if target == 'pil':\n","        img = FT.to_pil_image(img)\n","\n","    elif target == '[0, 255]':\n","        img = 255. * img\n","\n","    elif target == '[0, 1]':\n","        pass  # already in [0, 1]\n","\n","    elif target == '[-1, 1]':\n","        img = 2. * img - 1.\n","\n","    elif target == 'imagenet-norm':\n","        if img.ndimension() == 3:\n","            img = (img - imagenet_mean) / imagenet_std\n","        elif img.ndimension() == 4:\n","            img = (img - imagenet_mean_cuda) / imagenet_std_cuda\n","\n","    elif target == 'y-channel':\n","        img = torch.matmul(255. * img.permute(0, 2, 3, 1)[:, 4:-4, 4:-4, :], rgb_weights) / 255. + 16.\n","\n","    return img\n","\n","\n","def process_array(image_array, expand=True):\n","\n","    image_batch = image_array / 255.0\n","    if expand:\n","        image_batch = np.expand_dims(image_batch, axis=0)\n","    return image_batch\n","\n","\n","def process_output(output_tensor):\n","\n","    sr_img = output_tensor.clip(0, 1) * 255\n","    sr_img = np.uint8(sr_img)\n","    return sr_img\n","\n","\n","def pad_patch(image_patch, padding_size, channel_last=True):\n","\n","    if channel_last:\n","        return np.pad(\n","            image_patch,\n","            ((padding_size, padding_size), (padding_size, padding_size), (0, 0)),\n","            'edge',\n","        )\n","    else:\n","        return np.pad(\n","            image_patch,\n","            ((0, 0), (padding_size, padding_size), (padding_size, padding_size)),\n","            'edge',\n","        )\n","\n","\n","def unpad_patches(image_patches, padding_size):\n","    return image_patches[:, padding_size:-padding_size, padding_size:-padding_size, :]\n","\n","\n","def split_image_into_overlapping_patches(image_array, patch_size, padding_size=2):\n","    \n","    xmax, ymax, _ = image_array.shape\n","    x_remainder = xmax % patch_size\n","    y_remainder = ymax % patch_size\n","    \n","    # modulo here is to avoid extending of patch_size instead of 0\n","    x_extend = (patch_size - x_remainder) % patch_size\n","    y_extend = (patch_size - y_remainder) % patch_size\n","    \n","    # make sure the image is divisible into regular patches\n","    extended_image = np.pad(image_array, ((0, x_extend), (0, y_extend), (0, 0)), 'edge')\n","    \n","    # add padding around the image to simplify computations\n","    padded_image = pad_patch(extended_image, padding_size, channel_last=True)\n","    \n","    xmax, ymax, _ = padded_image.shape\n","    patches = []\n","    \n","    x_lefts = range(padding_size, xmax - padding_size, patch_size)\n","    y_tops = range(padding_size, ymax - padding_size, patch_size)\n","    \n","    for x in x_lefts:\n","        for y in y_tops:\n","            x_left = x - padding_size\n","            y_top = y - padding_size\n","            x_right = x + patch_size + padding_size\n","            y_bottom = y + patch_size + padding_size\n","            patch = padded_image[x_left:x_right, y_top:y_bottom, :]\n","            patches.append(patch)\n","    \n","    return np.array(patches), padded_image.shape\n","\n","\n","def stich_together(patches, padded_image_shape, target_shape, padding_size=4):\n","\n","    xmax, ymax, _ = padded_image_shape\n","    patches = unpad_patches(patches, padding_size)\n","    patch_size = patches.shape[1]\n","    n_patches_per_row = ymax // patch_size\n","    \n","    complete_image = np.zeros((xmax, ymax, 3))\n","    \n","    row = -1\n","    col = 0\n","    for i in range(len(patches)):\n","        if i % n_patches_per_row == 0:\n","            row += 1\n","            col = 0\n","        complete_image[\n","        row * patch_size: (row + 1) * patch_size, col * patch_size: (col + 1) * patch_size,:\n","        ] = patches[i]\n","        col += 1\n","    return complete_image[0: target_shape[0], 0: target_shape[1], :]\n","\n","\n","def load_inter_weights(psnr_path, gan_path, alpha=0.75):\n","    psnr_w = torch.load(psnr_path)\n","    gan_w = torch.load(gan_path)\n","\n","    inter_w = OrderedDict()\n","\n","    for k, w_psnr in psnr_w.items():\n","        w_gan = gan_w[k]\n","        inter_w[k] = (1 - alpha) * w_psnr + alpha * w_gan\n","\n","    return inter_w\n","\n","\n","def predict_by_patches(model, lr_img, batch_size=4, patches_size=192,\n","                       padding=15, scale=4):\n","    lr_image = np.array(lr_img)\n","    patches, p_shape = split_image_into_overlapping_patches(lr_image, patch_size=patches_size, \n","                                                            padding_size=padding)\n","    img = torch.FloatTensor(patches/255).permute((0,3,1,2)).to(device)\n","    img = convert_image(img, source='[0, 1]', target='imagenet-norm')\n","\n","    with torch.no_grad():\n","        torch.cuda.empty_cache()\n","        res = model(img[0:batch_size]).detach().cpu()\n","        for i in range(batch_size, img.shape[0], batch_size):\n","            res = torch.cat((res, model(img[i:i+batch_size]).detach().cpu()), 0)\n","\n","    sr_image = convert_image(res, source='[-1, 1]', target='[0, 1]').permute((0,2,3,1))\n","    np_sr_image = np.array(sr_image.data)\n","\n","    padded_size_scaled = tuple(np.multiply(p_shape[0:2], scale)) + (3,)\n","    scaled_image_shape = tuple(np.multiply(lr_image.shape[0:2], scale)) + (3,)\n","    np_sr_image = stich_together(np_sr_image, padded_image_shape=padded_size_scaled,\n","                            target_shape=scaled_image_shape, padding_size=padding * scale)\n","    sr_img = Image.fromarray((np_sr_image*255).astype(np.uint8))\n","    return sr_img\n","\n","checkpoint_gan = 'netG_20blocks_comprandom_dsV4_4x_V3bicubic.pth'\n","checkpoint_psnr = 'RRDBNet_PSNR_20blocks_comprandom_dsV4_4x_V3bicubic.pth'\n","model = Generator(num_rrdb_blocks=20)\n","inter_w = load_inter_weights(checkpoint_psnr, checkpoint_gan, alpha=0.95)\n","model.load_state_dict(inter_w)\n","model.cuda()\n","print('Модель готова к работе')"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Модель готова к работе\n"]}]},{"cell_type":"code","metadata":{"id":"CHk56wAJPnqk"},"source":["from IPython.display import display\n","import os"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"aAX4vS4BPp6j"},"source":["##Работа с картинками"]},{"cell_type":"code","metadata":{"id":"MV7TnY0emaYF"},"source":["#@title Размер картинки (рекомендованно 1024*1024)\n","pix1024=True #@param {type:'boolean'}\n","pix4096=False #@param {type:'boolean'}\n","\n","times=0\n","if pix1024:\n","  times=1\n","elif pix4096:\n","  times=2"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"sLQDjhqKQAZE"},"source":["#@title Генерация изображения\n","#@markdown На изображениях с размером более 1024 пикселей модель может работать очень долго!\n","\n","for j in range(len(data)):\n","  for i in range(451):\n","   try:\n","    lr_img = Image.open(f'{path}/{j:03}_{i:04}.png').convert('RGB')\n","   except FileNotFoundError:\n","    continue\n","\n","   for t in range(times):\n","    result = predict_by_patches(model, lr_img)\n","    lr_img = result\n","\n","   result.save(f'{path}/{j:03}_{i:04}.png')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"02ZbcWw5YYnU"},"source":["# Сбор видео"]},{"cell_type":"code","metadata":{"id":"LGCV6hM1RlmB","executionInfo":{"status":"ok","timestamp":1634406102140,"user_tz":-180,"elapsed":237,"user":{"displayName":"Дарья Курганская","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"10001247416734268166"}}},"source":["from PIL import Image\n","import numpy as np"],"execution_count":6,"outputs":[]},{"cell_type":"code","metadata":{"id":"BB8_4iTcXWtO","executionInfo":{"status":"ok","timestamp":1634406112699,"user_tz":-180,"elapsed":9594,"user":{"displayName":"Дарья Курганская","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"10001247416734268166"}}},"source":["import os \n","\n","i=0\n","for file in os.listdir(path):\n","  i+=1"],"execution_count":7,"outputs":[]},{"cell_type":"code","metadata":{"id":"jSGhxCiUbDK9","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1634406149147,"user_tz":-180,"elapsed":832,"user":{"displayName":"Дарья Курганская","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"10001247416734268166"}},"outputId":"750817d4-1fbf-4cb1-8e98-5ea7581ea64e"},"source":["i"],"execution_count":8,"outputs":[{"output_type":"execute_result","data":{"text/plain":["1555"]},"metadata":{},"execution_count":8}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bTo2oHYaPTE2","executionInfo":{"status":"ok","timestamp":1634406854710,"user_tz":-180,"elapsed":298,"user":{"displayName":"Дарья Курганская","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"10001247416734268166"}},"outputId":"da6c4fcd-6999-48f1-d444-8a228e51fad4"},"source":["%cd MyDrive"],"execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive\n"]}]},{"cell_type":"code","metadata":{"id":"mFo5vz0UYBrF","colab":{"base_uri":"https://localhost:8080/","height":66,"referenced_widgets":["4092caad911f4d8b961289f71d075e45","493f31b3395f4ddca7533ff77b7e4162","a1294f83193848febe4b7a6c87816f06","534246bc28b245278c345a3d1272d373","c05e9cd1c28b49d89339b2d014ee9ff2","e8eca0368a454554993a1f3b481e5f3a","9642a91e2189471e92aee68365281e9c","570ec0731ab64ebb941d16a2038e7587","aea9d5d91ad34820ae5687e0dacdbbf8","6d474411acf7471eab82bb776a4d0520","c609fb3495744ab9a53bd2831475107d"]},"executionInfo":{"status":"ok","timestamp":1634406979187,"user_tz":-180,"elapsed":110901,"user":{"displayName":"Дарья Курганская","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"10001247416734268166"}},"outputId":"278831ac-356f-44c7-be45-963158c662e3"},"source":["init_frame = 0 \n","last_frame = i #сколько кадров в фильме\n","'''\n","min_fps = 10\n","max_fps = 30\n","\n","total_frames = last_frame-init_frame\n","\n","length = data[-1]['start'] #Сколько видео будет длиться (та же продолжительность, что и у файла voice.wav)\n","'''\n","frames = []\n","\n","for j in range(30):\n","  for i in range(451):\n","    try: \n","      filename = f\"{path}/{j:03}_{i:04}.png\"\n","      frames.append(Image.open(filename))\n","    except FileNotFoundError:\n","      continue\n","\n","\n","#fps = np.clip(total_frames/length,min_fps,max_fps)\n","fps = 10\n","from subprocess import Popen, PIPE\n","p = Popen(['ffmpeg', '-y', '-f', 'image2pipe', '-vcodec', 'png', '-r', str(fps), '-i', '-', '-vcodec', 'libx264', '-r', str(fps), '-pix_fmt', 'yuv420p', '-crf', '17', '-preset', 'veryslow', 'th1-1.mp4'], stdin=PIPE)\n","for im in tqdm(frames):\n","  im.save(p.stdin, 'PNG')\n","p.stdin.close()\n","\n","p.wait()\n","print(\"Видео готово\")"],"execution_count":13,"outputs":[{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"4092caad911f4d8b961289f71d075e45","version_minor":0,"version_major":2},"text/plain":["  0%|          | 0/1555 [00:00<?, ?it/s]"]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Видео готово\n"]}]},{"cell_type":"markdown","metadata":{"id":"iMso2kipxDaX"},"source":["##(опционнально) Увеличить разрешение 2"]},{"cell_type":"code","metadata":{"id":"WvmjDjhKxB0K"},"source":["import cv2\n","import numpy as np \n","\n","i=0\n","cap=cv2.VideoCapture('/content/video.mp4')\n","fourcc=cv2.VideoWriter_fourcc(*'XVID') \n","out= cv2.VideoWriter('/content/videofinal.mp4',fourcc,11,(4096,4096))\n","\n","\n","while True:\n","  ret, frame=cap.read()\n","  if ret==True :\n","    b=cv2.resize(frame, (4096,4096), interpolation=cv2.INTER_NEAREST)\n","    out.write(b)\n","    i+=1\n","    if i%100==0:\n","      print(i)\n","  else:\n","    break\n","\n","cap.release()\n","out.release()\n","cv2.destroyAllWindows()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Y0e8pHyJmi7s","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1633459659592,"user_tz":-180,"elapsed":269,"user":{"displayName":"Дарья Курганская","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"00136910481207940866"}},"outputId":"210bb867-5255-433a-cbc3-842dabf6f52e"},"source":["# @title Скачать видео\n","from google.colab import files\n","files.download(\"videofinal.mp4\")"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"application/javascript":["\n","    async function download(id, filename, size) {\n","      if (!google.colab.kernel.accessAllowed) {\n","        return;\n","      }\n","      const div = document.createElement('div');\n","      const label = document.createElement('label');\n","      label.textContent = `Downloading \"${filename}\": `;\n","      div.appendChild(label);\n","      const progress = document.createElement('progress');\n","      progress.max = size;\n","      div.appendChild(progress);\n","      document.body.appendChild(div);\n","\n","      const buffers = [];\n","      let downloaded = 0;\n","\n","      const channel = await google.colab.kernel.comms.open(id);\n","      // Send a message to notify the kernel that we're ready.\n","      channel.send({})\n","\n","      for await (const message of channel.messages) {\n","        // Send a message to notify the kernel that we're ready.\n","        channel.send({})\n","        if (message.buffers) {\n","          for (const buffer of message.buffers) {\n","            buffers.push(buffer);\n","            downloaded += buffer.byteLength;\n","            progress.value = downloaded;\n","          }\n","        }\n","      }\n","      const blob = new Blob(buffers, {type: 'application/binary'});\n","      const a = document.createElement('a');\n","      a.href = window.URL.createObjectURL(blob);\n","      a.download = filename;\n","      div.appendChild(a);\n","      a.click();\n","      div.remove();\n","    }\n","  "],"text/plain":["<IPython.core.display.Javascript object>"]},"metadata":{}},{"output_type":"display_data","data":{"application/javascript":["download(\"download_cc6483e6-950b-44a4-ba7e-382f9489e7af\", \"videofinal.mp4\", 203147880)"],"text/plain":["<IPython.core.display.Javascript object>"]},"metadata":{}}]},{"cell_type":"markdown","metadata":{"id":"83x5lq63R_DM"},"source":["#Обьединение со звуком"]},{"cell_type":"markdown","metadata":{"id":"AVIP6eVA5tOH"},"source":["Загрузите, пожалуйста, файлы voice.wav и music.wav"]},{"cell_type":"code","metadata":{"id":"krKbIsEXquQE"},"source":["!pip install pydub\n","!pip install ffmpeg\n","from google.colab import files"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ua8dwxnGfomC"},"source":["from pydub import AudioSegment\n","sound1 = AudioSegment.from_file(\"/content/voice.wav\", format=\"wav\")\n","\n","sound2 = AudioSegment.from_file(\"/content/music.wav\", format=\"wav\")\n","sound2 = sound2 - 6 # на 6dB тише\n","overlay = sound1.overlay(sound2, position=0)\n","overlay.export('/content/audio.wav', format='wav')\n","!ffmpeg  -i /content/audio.wav -i /content/video.mp4 /content/DigiTale.mp4\n","files.download('/content/DigiTale.mp4')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"MfIjlPpuRMHI"},"source":["#Фильм готов! Наслаждайтесь просмотром!"]}]}